{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dec9a320",
   "metadata": {},
   "source": [
    "This notebook can be used to learn to fit EBMRegressor and to save key attributes of the model. Then we load the key attributes of the model and see if we can replicate the predition without using interpretML library. High Level comments on the parameters saved and how the prediction algorithm works is also present. Be sure to read the comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0b99fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import GroupShuffleSplit \n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac37732",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88448ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_file = r'Related_files_L1_input.json'\n",
    "prediction_parameter_file = 'prediction_parameter_file.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3498222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_json(input_file,lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18c46594",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = GroupShuffleSplit(test_size=.30, n_splits=1, random_state = 7)\n",
    "split = splitter.split(df, groups=df['topic_id'])\n",
    "train_inds, test_inds = next(split)\n",
    "\n",
    "train_df = df.iloc[train_inds]\n",
    "test_df = df.iloc[test_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21125832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train data 7351\n",
      "Length of test data 3088\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of train data\" , len(train_df))\n",
    "print(\"Length of test data\" , len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9720603e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7351, 7351)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keeping it simple and using 7 features\n",
    "train_features =['TitleQuery_TokenMatchScore','TitleQuery_FullMatchScore','EditorMatchScore','ViewsLifeTimeScore',\n",
    "                 'Title_UnmatchedTokenCountScore','FileTypeScore','Title_GoodTitleKeywordsScore',]\n",
    "X_train=train_df[train_features]\n",
    "X_test=test_df[train_features]\n",
    "y_train=train_df[['RelatedDocumentLabel']]\n",
    "y_test=test_df[['RelatedDocumentLabel']]\n",
    "(len(X_train),len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85e567ab",
   "metadata": {},
   "source": [
    "## Fit EBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c9d89992",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExplainableBoostingRegressor(feature_names=['TitleQuery_TokenMatchScore',\n",
       "                                            'TitleQuery_FullMatchScore',\n",
       "                                            'EditorMatchScore',\n",
       "                                            'ViewsLifeTimeScore',\n",
       "                                            'Title_UnmatchedTokenCountScore',\n",
       "                                            'FileTypeScore',\n",
       "                                            'Title_GoodTitleKeywordsScore',\n",
       "                                            'TitleQuery_FullMatchScore x '\n",
       "                                            'ViewsLifeTimeScore',\n",
       "                                            'TitleQuery_TokenMatchScore x '\n",
       "                                            'ViewsLifeTimeScore',\n",
       "                                            'TitleQuery_FullMatchScore x '\n",
       "                                            'T...\n",
       "                                            'FileTypeScore',\n",
       "                                            'ViewsLifeTimeScore x '\n",
       "                                            'Title_UnmatchedTokenCountScore',\n",
       "                                            'EditorMatchScore x '\n",
       "                                            'ViewsLifeTimeScore'],\n",
       "                             feature_types=['continuous', 'categorical',\n",
       "                                            'categorical', 'continuous',\n",
       "                                            'continuous', 'continuous',\n",
       "                                            'continuous', 'interaction',\n",
       "                                            'interaction', 'interaction',\n",
       "                                            'interaction', 'interaction',\n",
       "                                            'interaction', 'interaction',\n",
       "                                            'interaction', 'interaction',\n",
       "                                            'interaction'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "ebm = ExplainableBoostingRegressor()\n",
    "ebm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db2cf5c",
   "metadata": {},
   "source": [
    "## Predict scores on a small test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e1fea753",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_small = X_test[0:3]\n",
    "scores  = ebm.predict(X_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9cdfe1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ex,explanation_ex = ebm.predict_and_contrib(X_test_small)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "51760c1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores == scores_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec973bd",
   "metadata": {},
   "source": [
    "## Save required parameters for independent predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ac3df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_col_bin_edges_ = dict()\n",
    "for k,v in ebm.preprocessor_.col_bin_edges_.items():\n",
    "    preprocessor_col_bin_edges_[k] = v.tolist()\n",
    "\n",
    "pair_preprocessor_col_bin_edges_ = dict()\n",
    "for k,v in ebm.pair_preprocessor_.col_bin_edges_.items():\n",
    "    pair_preprocessor_col_bin_edges_[k] = v.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bf2e7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "required_data_for_prediction= dict()\n",
    "required_data_for_prediction['feature_groups_']= ebm.feature_groups_ # 7 individual single features plus 10 interactions\n",
    "required_data_for_prediction['additive_terms_']= [x.tolist() for x in ebm.additive_terms_]  # for each bin in continuos variable or each categorical value, it has a number or value associated with it. Shape [# of features +feature pair,bins of each category]\n",
    "required_data_for_prediction['intercept_']= ebm.intercept_\n",
    "required_data_for_prediction['preprocessor_col_types_']= ebm.preprocessor_.col_types_ # tells whether each col is continuous, ordinal or categorical.\n",
    "required_data_for_prediction['preprocessor_col_bin_edges_']= preprocessor_col_bin_edges_ # each continuous variable has bins for it as key value pair. the anything less than that value but greater than previous one belogns to that bin\n",
    "required_data_for_prediction['preprocessor_col_mapping_']= ebm.preprocessor_.col_mapping_ # # features 1 & 2 are categorical. For these categorial values, if mapping: {'0.0': 1, '1.0': 2} then col_value of 0 & 0.0 will change to 1 , dtype:float\n",
    "required_data_for_prediction['preprocessor_binning']= ebm.preprocessor_.binning  # in prediction this is mainly used for dp stuff to see if we need to go to private if block\n",
    "required_data_for_prediction['preprocessor_missing_str']= ebm.preprocessor_.missing_str # what missing value is represented as in categorial variables. In preprocessing missing_str is mapped to 0\n",
    "required_data_for_prediction['feature_types']=ebm.feature_types # whether type is continuous,categorical or categorical\n",
    "required_data_for_prediction['feature_names'] = ebm.feature_names # feature names. Include single features and pairwise interactions\n",
    "required_data_for_prediction['interactions'] = ebm.interactions # no of pair-wise interactions in the model\n",
    "required_data_for_prediction['pair_preprocessor_col_types_']= ebm.pair_preprocessor_.col_types_\n",
    "required_data_for_prediction['pair_preprocessor_col_bin_edges_']= pair_preprocessor_col_bin_edges_\n",
    "required_data_for_prediction['pair_preprocessor_col_mapping_']= ebm.pair_preprocessor_.col_mapping_\n",
    "required_data_for_prediction['pair_preprocessor_binning']= ebm.pair_preprocessor_.binning\n",
    "required_data_for_prediction['pair_preprocessor_missing_str']= ebm.pair_preprocessor_.missing_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "146e2294",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(prediction_parameter_file,'w',encoding='utf-8') as paf:\n",
    "    paf.write(json.dumps(required_data_for_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce12ff",
   "metadata": {},
   "source": [
    "## Predict without ebm or interpretMl Library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad3e564",
   "metadata": {},
   "source": [
    "- First in the parent folder create a folder called lib and copy the dll lib_ebm_native_win_x64.dll  . It has a function required for assign the feature value to a particular bin for that feature\n",
    "- Create a folder glassbox/ebm and add the file internal.py . I think this path & structure is hard coded somewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47f1691e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glassbox.ebm.internal import Native \n",
    "#from \"C:\\InstalledPrograms\\Anaconda3\\Lib\\site-packages\\interpret_core-0.2.7-py3.8.egg\\interpret\\glassbox\\ebm\\internal.py\"\n",
    "# create folder lib and add dlls from C:\\InstalledPrograms\\Anaconda3\\Lib\\site-packages\\interpret_core-0.2.7-py3.8.egg\\interpret\\lib\n",
    "import numpy as np\n",
    "import numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af1beb6",
   "metadata": {},
   "source": [
    "### Function to arrange & clean features like train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "421a16c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.core.generic import NDFrame\n",
    "# TODO: Docs for unify_data.\n",
    "def unify_data(data, labels=None, feature_names=None, feature_types=None, missing_data_allowed=False):\n",
    "    \"\"\" Attempts to unify data into a numpy array with feature names and types.\n",
    "\n",
    "    If it cannot unify, returns the original data structure.\n",
    "\n",
    "    Args:\n",
    "        data:\n",
    "        labels:\n",
    "        feature_names:\n",
    "        feature_types:\n",
    "\n",
    "    Returns:\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO: Clean up code to have less duplication.\n",
    "    if isinstance(data, NDFrame):\n",
    "        # NOTE: Workaround for older versions of pandas.\n",
    "        try:\n",
    "            new_data = data.to_numpy()\n",
    "        except AttributeError:  # pragma: no cover\n",
    "            new_data = data.values\n",
    "\n",
    "        if feature_names is None:\n",
    "            new_feature_names = list(data.columns)\n",
    "        else:\n",
    "            new_feature_names = feature_names\n",
    "\n",
    "        if feature_types is None:\n",
    "            # unique_counts = np.apply_along_axis(lambda a: len(set(a)), axis=0, arr=data)\n",
    "            bool_indicator = [data[col].isin([np.nan, 0, 1]).all() for col in data.columns]\n",
    "            new_feature_types = [\n",
    "                _assign_feature_type(feature_type, bool_indicator[index])\n",
    "                for index, feature_type in enumerate(data.dtypes)\n",
    "            ]\n",
    "        else:\n",
    "            new_feature_types = feature_types\n",
    "    elif isinstance(data, list):\n",
    "        new_data = np.array(data)\n",
    "\n",
    "        new_feature_names = _get_new_feature_names(new_data, feature_names)\n",
    "        new_feature_types = _get_new_feature_types(\n",
    "            new_data, feature_types, new_feature_names\n",
    "        )\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        new_data = data\n",
    "\n",
    "        new_feature_names = _get_new_feature_names(data, feature_names)\n",
    "        new_feature_types = _get_new_feature_types(\n",
    "            data, feature_types, new_feature_names\n",
    "        )\n",
    "    elif sp.sparse.issparse(data):\n",
    "        # Add warning message for now prior to converting the data to dense format\n",
    "        warn_msg = (\n",
    "            \"Sparse data not fully supported, will be densified for now, may cause OOM\"\n",
    "        )\n",
    "        warnings.warn(warn_msg, RuntimeWarning)\n",
    "        new_data = data.toarray()\n",
    "\n",
    "        new_feature_names = _get_new_feature_names(new_data, feature_names)\n",
    "        new_feature_types = _get_new_feature_types(\n",
    "            new_data, feature_types, new_feature_names\n",
    "        )\n",
    "    else:  # pragma: no cover\n",
    "        msg = \"Could not unify data of type: {0}\".format(type(data))\n",
    "        log.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    new_labels = unify_vector(labels)\n",
    "\n",
    "    # NOTE: Until missing handling is introduced, all methods will fail at data unification stage if present.\n",
    "    new_data_has_na = (\n",
    "        True if new_data is not None and pd.isnull(new_data).any() else False\n",
    "    )\n",
    "    new_labels_has_na = (\n",
    "        True if new_labels is not None and pd.isnull(new_labels).any() else False\n",
    "    )\n",
    "\n",
    "    if (new_data_has_na and not missing_data_allowed) or new_labels_has_na:\n",
    "        msg = \"Missing values are currently not supported.\"\n",
    "        log.error(msg)\n",
    "        raise ValueError(msg)\n",
    "\n",
    "    return new_data, new_labels, new_feature_names, new_feature_types\n",
    "\n",
    "def unify_vector(data):\n",
    "    if data is None:\n",
    "        return None\n",
    "\n",
    "    if isinstance(data, Series):\n",
    "        new_data = data.values\n",
    "    elif isinstance(data, np.ndarray):\n",
    "        if data.ndim > 1:\n",
    "            new_data = data.ravel()\n",
    "        else:\n",
    "            new_data = data\n",
    "    elif isinstance(data, list):\n",
    "        new_data = np.array(data)\n",
    "    elif isinstance(data, NDFrame) and data.shape[1] == 1:\n",
    "        new_data = data.iloc[:, 0].values\n",
    "    else:  # pragma: no cover\n",
    "        msg = \"Could not unify data of type: {0}\".format(type(data))\n",
    "        log.warning(msg)\n",
    "        raise Exception(msg)\n",
    "\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f39263",
   "metadata": {},
   "source": [
    "### Function to transform the input feature value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91260fe5",
   "metadata": {},
   "source": [
    "It takes in feature values and converts them to bin indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e961506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessor_transform(X_preprocessor_transform_input, col_bin_edges_, col_types_, col_mapping_, binning, missing_str):\n",
    "    missing_constant = 0\n",
    "    unknown_constant = -1\n",
    "    native = Native.get_native_singleton()\n",
    "    X_new = np.copy(X_preprocessor_transform_input)\n",
    "    if issubclass(X_preprocessor_transform_input.dtype.type, np.unsignedinteger):\n",
    "        X_new = X_new.astype(np.int64)\n",
    "\n",
    "    for col_idx in range(X_preprocessor_transform_input.shape[1]):\n",
    "            col_type = col_types_[col_idx]\n",
    "            col_data = X_preprocessor_transform_input[:, col_idx]\n",
    "\n",
    "            if col_type == \"continuous\":\n",
    "                col_data = col_data.astype(float)\n",
    "                cuts = np.array(col_bin_edges_[str(col_idx)])\n",
    "                discretized = native.discretize(col_data, cuts) # this basically says which bucket the col_data \n",
    "                #value belongs to. If col_value is 1 and the buckets are [0.5,1.5,2.5], then the discretized value will be 1 for that value\n",
    "                X_new[:, col_idx] = discretized\n",
    "\n",
    "            elif col_type == \"ordinal\":\n",
    "                mapping = col_mapping_[col_idx].copy()\n",
    "                vec_map = np.vectorize(\n",
    "                    lambda x: mapping[x] if x in mapping else unknown_constant\n",
    "                )\n",
    "                X_new[:, col_idx] = vec_map(col_data)\n",
    "            elif col_type == \"categorical\":\n",
    "                mapping = col_mapping_[str(col_idx)].copy()\n",
    "\n",
    "                # Use \"DPOther\" bin when possible to handle unknown values during DP.\n",
    "                if \"private\" in binning:\n",
    "                    for key, val in mapping.items():\n",
    "                        if key == \"DPOther\": \n",
    "                            unknown_constant = val\n",
    "                            missing_constant = val\n",
    "                            break\n",
    "                        else: # If DPOther keyword doesn't exist, revert to standard encoding scheme\n",
    "                            missing_constant = 0\n",
    "                            unknown_constant = -1\n",
    "\n",
    "                if isinstance(missing_str, list):\n",
    "                    for val in missing_str:\n",
    "                        mapping[val] = missing_constant\n",
    "                else:\n",
    "                    mapping[missing_str] = missing_constant\n",
    "\n",
    "                col_data = col_data.astype('U')\n",
    "                X_new[:, col_idx] = np.fromiter(\n",
    "                    (mapping.get(x, unknown_constant) for x in col_data), dtype=np.int64, count=X_preprocessor_transform_input.shape[0]\n",
    "                )\n",
    "\n",
    "    return X_new.astype(np.int64)      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9a0f2d",
   "metadata": {},
   "source": [
    "### Preproccess individual and pair features separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7586d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_para = dict()\n",
    "with open(prediction_parameter_file,'r') as f:\n",
    "    for line in f:\n",
    "        pred_para=json.loads(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "283a2de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_orig, _, _, _ = unify_data(X_test_small, None, pred_para['feature_names'], pred_para['feature_types'], missing_data_allowed=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ac4aad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# common keys\n",
    "feature_names = pred_para['feature_names'] # feature names. Include single features and pairwise interactions\n",
    "interactions =pred_para['interactions'] # no of pair-wise interactions in the model\n",
    "feature_types = pred_para['feature_types'] # whether type is continuous,categorical or categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af259a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys specific to single feature GAMS\n",
    "# all preprocessor keys are only for single features\n",
    "preprocessor_col_bin_edges_ = pred_para['preprocessor_col_bin_edges_'] # each continuous variable has bins for it as key value pair. the anything less than that value but greater than previousone belogns to that bin\n",
    "preprocessor_col_types_ = pred_para['preprocessor_col_types_'] # tells whether each col is continuous, ordinal or categorical. \n",
    "preprocessor_col_mapping_=pred_para['preprocessor_col_mapping_'] # features 1 & 2 are categorical. For these categorial values, if mapping: {'0.0': 1, '1.0': 2} then col_value of 0 & 0.0 will change to 1 , dtype:float\n",
    "preprocessor_binning= pred_para['preprocessor_binning']  # in prediction this is mainly used for dp stuff to see if we need to go to private if block\n",
    "preprocessor_missing_str = pred_para['preprocessor_missing_str'] # what missing value is represented as in categorial variables. In preprocessing this is mapped to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c21cc86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keys specific to pairwise feature GAMS\n",
    "pair_preprocessor_col_bin_edges_ = pred_para['pair_preprocessor_col_bin_edges_']\n",
    "pair_preprocessor_col_types_ = pred_para['pair_preprocessor_col_types_']\n",
    "pair_preprocessor_col_mapping_=pred_para['pair_preprocessor_col_mapping_']\n",
    "pair_preprocessor_binning= pred_para['pair_preprocessor_binning']\n",
    "pair_preprocessor_missing_str = pred_para['pair_preprocessor_missing_str']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0af6d857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual features\n",
    "X = preprocessor_transform(X_orig, preprocessor_col_bin_edges_, preprocessor_col_types_, preprocessor_col_mapping_, preprocessor_binning, preprocessor_missing_str)\n",
    "X = np.ascontiguousarray(X.T) # each column is a record with shape(#of features,# of records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bbdb377e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if interactions >0 : # max interactions are set in max_interaction_bins. Default is 32\n",
    "    X_pair = preprocessor_transform(X_orig, pair_preprocessor_col_bin_edges_, pair_preprocessor_col_types_, pair_preprocessor_col_mapping_, pair_preprocessor_binning, pair_preprocessor_missing_str)\n",
    "    X_pair = np.ascontiguousarray(X_pair.T) # each column is a record with shape(#of features,# of records)\n",
    "else:\n",
    "    X_pair = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d09a554",
   "metadata": {},
   "source": [
    "## Predict & score Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19f2290",
   "metadata": {},
   "source": [
    "The algorithm at a high level is \n",
    "- Using information from preproccessor & pair_preprocessor above , for each feature we can discretize it into bins\n",
    "- additive_terms_ contains the value of for each bin for each feature & feature-pair.\n",
    "- Score is simply the sum of score values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8437714b",
   "metadata": {},
   "source": [
    "To Do:<br>\n",
    "- Find out how pair features are discretized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "289706c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_by_feature_group(X, X_pair, feature_groups, model):\n",
    "        for set_idx, feature_group in enumerate(feature_groups):\n",
    "            tensor = model[set_idx]\n",
    "\n",
    "            # Get the current column(s) to process\n",
    "            feature_idxs = feature_group\n",
    "\n",
    "            if X_pair is not None:\n",
    "                sliced_X = X[feature_idxs, :] if len(feature_group) == 1 else X_pair[feature_idxs, :]\n",
    "            else:\n",
    "                sliced_X = X[feature_idxs, :]\n",
    "\n",
    "            scores = tensor[tuple(sliced_X)]\n",
    "\n",
    "            # Reset scores from unknown (not missing!) indexes to 0\n",
    "            # this assumes all logits are zero weighted centered, and ideally tensors are purified\n",
    "\n",
    "            unknowns = (sliced_X < 0).any(axis=0) \n",
    "            scores[unknowns] = 0# negative values are replaced with zero\n",
    "\n",
    "            yield set_idx, feature_group, scores\n",
    "            \n",
    "def decision_function(X, X_pair, feature_groups, model, intercept):\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(X.shape[0], 1)\n",
    "\n",
    "        # Initialize empty vector for predictions\n",
    "        if isinstance(intercept, numbers.Number) or len(intercept) == 1:\n",
    "            score_vector = np.empty(X.shape[1]) # assign empty random values with shape equal to number of records\n",
    "            \n",
    "        else:\n",
    "            score_vector = np.empty((X.shape[1], len(intercept)))\n",
    "\n",
    "        np.copyto(score_vector, intercept) # assign the value of intercept \n",
    "        \n",
    "        # Generate prediction scores\n",
    "        scores_gen = scores_by_feature_group(\n",
    "            X, X_pair, feature_groups, model\n",
    "        ) # value of each feature + feature pair according to the bin it was placed in \n",
    "        for _, _, scores in scores_gen:\n",
    "            score_vector += scores\n",
    "\n",
    "        if not np.all(np.isfinite(score_vector)):  # pragma: no cover\n",
    "            msg = \"Non-finite values present in log odds vector.\"\n",
    "            log.error(msg)\n",
    "            raise Exception(msg)\n",
    "\n",
    "        return score_vector\n",
    "\n",
    "\n",
    "\n",
    "def regressor_predict(X, X_pair, feature_groups, model, intercept):\n",
    "        scores = decision_function(X, X_pair, feature_groups, model, intercept)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e05414e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_groups_ = pred_para['feature_groups_'] # 7 individual single features plus 10 interactions\n",
    "additive_terms_ = [np.array(x) for x in  pred_para['additive_terms_']] # for each bin in continuos variable or each categorical value, it has a number or value associated with it. Shape [# of features +feature pair,bins of each category]\n",
    "intercept_ = pred_para['intercept_']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c64ccf14",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_wihtout_interpret = regressor_predict(X, X_pair, feature_groups_, additive_terms_, intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1616653c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores == scores_wihtout_interpret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9808dcbe",
   "metadata": {},
   "source": [
    "## Mostly similar function which also keeps record of explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "19323f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_function_and_explain(X, X_pair, feature_groups, model, intercept):\n",
    "    if X.ndim == 1:\n",
    "        X = X.reshape(X.shape[0], 1)\n",
    "\n",
    "    # Initialize empty vector for predictions and explanations\n",
    "    if isinstance(intercept, numbers.Number) or len(intercept) == 1:\n",
    "        score_vector = np.empty(X.shape[1])\n",
    "    else:\n",
    "        score_vector = np.empty((X.shape[1], len(intercept)))\n",
    "\n",
    "    np.copyto(score_vector, intercept)\n",
    "\n",
    "    n_interactions = sum(len(fg) > 1 for fg in feature_groups)\n",
    "    explanations = np.empty((X.shape[1], X.shape[0] + n_interactions))\n",
    "\n",
    "    # Generate prediction scores\n",
    "    scores_gen = scores_by_feature_group(\n",
    "        X, X_pair, feature_groups, model\n",
    "    )\n",
    "    for set_idx, _, scores in scores_gen:\n",
    "        score_vector += scores\n",
    "        explanations[:, set_idx] = scores\n",
    "\n",
    "    if not np.all(np.isfinite(score_vector)):  # pragma: no cover\n",
    "        msg = \"Non-finite values present in log odds vector.\"\n",
    "        log.error(msg)\n",
    "        raise Exception(msg)\n",
    "\n",
    "    return score_vector, explanations\n",
    "    \n",
    "def regressor_predict_and_contrib(X, X_pair, feature_groups, model, intercept):\n",
    "    scores, explanations = decision_function_and_explain(\n",
    "        X,\n",
    "        X_pair,\n",
    "        feature_groups,\n",
    "        model,\n",
    "        intercept\n",
    "    )\n",
    "    return scores, explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "00f1d0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_wihtout_interpret_e, explanations_wihtout_interpret = regressor_predict_and_contrib(X, X_pair, feature_groups_, additive_terms_, intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3c98b3e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores == scores_wihtout_interpret_e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "05a5af67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True],\n",
       "       [ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explanation_ex == explanations_wihtout_interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa5e67b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_37",
   "language": "python",
   "name": "py_37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
